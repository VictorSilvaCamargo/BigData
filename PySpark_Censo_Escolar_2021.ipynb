{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorSilvaCamargo/BigData/blob/main/PySpark_Censo_Escolar_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_5QOFMbISmZ",
        "outputId": "3c953e1e-2c4a-4261-d3d4-f6b6a73c9fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "MjcW2VpdIaJw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"PySpark\").master('local[*]').getOrCreate()\n",
        "\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "UxWY8Mo_Ib2p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "rdd = sc.textFile(\"censo_escolar_2021.csv\")\n",
        "\n",
        "\n",
        "rdd_filtered = rdd.map(lambda x: x.lower().replace(\".\",\"\").replace(\",\",\"\"))\n",
        "\n",
        "\n",
        "rdd_filtered = rdd_filtered.filter(lambda x:x.split(\";\")[0] != \"nu_ano_censo\")\n"
      ],
      "metadata": {
        "id": "5rnIKK2GIdhY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rdd_curitiba = rdd_filtered.filter(lambda x: x.split(\";\")[6] == \"curitiba\")\n",
        "\n",
        "\n",
        "total_escolas = rdd_curitiba.count()\n",
        "\n",
        "\n",
        "with open('Escolas_Curitiba.txt', 'w') as f:\n",
        "    f.write(str(total_escolas) + '\\n')\n"
      ],
      "metadata": {
        "id": "Jxn9BVBzIkZU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "rdd_chaveValor = rdd_filtered.map(lambda x: (x.split(\";\")[1], 1))\n",
        "\n",
        "rdd_count = rdd_chaveValor.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "rdd_escolaPorRegiao = rdd_count.sortBy(lambda x: x[0], ascending=True)\n",
        "\n",
        "rdd_escolaPorRegiao.collect()\n",
        "\n",
        "dados = rdd_escolaPorRegiao.collect()\n",
        "\n",
        "with open('Escolas_Por_Regiao.txt', 'w') as f:\n",
        "    for linha in dados:\n",
        "        f.write(str(linha) + '\\n')\n"
      ],
      "metadata": {
        "id": "owglLFlTInsK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tuple(s):\n",
        "    fields = s.split(\";\")\n",
        "    qt_doc_bas = int(fields[338]) if fields[338] else 0\n",
        "    qt_doc_fund = int(fields[342]) if fields[342] else 0\n",
        "    qt_doc_med = int(fields[345]) if fields[345] else 0\n",
        "    return ((fields[14], fields[6]),(qt_doc_bas + qt_doc_fund + qt_doc_med))\n",
        "\n",
        "\n",
        "rdd_escola = rdd_filtered.map(build_tuple)\n",
        "rdd_escola_sorted = rdd_escola.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "rdd_escola_sorted.take(1)\n",
        "\n",
        "primeira_linha = rdd_escola_sorted.first()\n",
        "\n",
        "with open('Primeira_Linha.txt', 'w') as f:\n",
        "    f.write(str(primeira_linha))"
      ],
      "metadata": {
        "id": "MNioSzgNIsQj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "def build_tuple(s):\n",
        "  fields = s.split(\";\")\n",
        "  qt_mat_med = int(fields[305]) if fields[305] else 0\n",
        "  return (fields[1], (qt_mat_med, 1))\n",
        "\n",
        "rdd_medio = rdd_filtered.map(build_tuple)\n",
        "rdd_medio_media = rdd_medio.reduceByKey(lambda x,y:(x[0] + y[0], x[1] + y[1]))\n",
        "rdd_medio_media = rdd_medio_media.map(lambda x:(x[0], x[1][0] / x[1][1]))\n",
        "rdd_medio_media.collect()\n",
        "\n",
        "rdd_dados = rdd_medio_media.collect()\n",
        "\n",
        "with open('Media_Matriculas.txt', 'w') as f:\n",
        "    for linha in rdd_dados:\n",
        "        f.write(str(linha) + '\\n')"
      ],
      "metadata": {
        "id": "1R850DZRIuER"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "def build_tuple(s):\n",
        "  fields = s.split(\";\")\n",
        "  tp_localizacao = int(fields[17]) if fields[17] else 0\n",
        "  tp_dependendia = int(fields[15]) if fields[15] else 0\n",
        "  return ((tp_localizacao, tp_dependendia), 1)\n",
        "\n",
        "rdd_escola_por_localizacao = rdd_filtered.map(build_tuple)\n",
        "\n",
        "rdd_teste = rdd_escola_por_localizacao.reduceByKey(lambda x, y: x + y)\n",
        "rdd_teste = rdd_teste.sortByKey()\n",
        "\n",
        "rdd_teste.collect()\n",
        "\n",
        "rdd_dados = rdd_teste.collect()\n",
        "\n",
        "\n",
        "with open('Quantidade_Por_Localização_Dependencia.txt', 'w') as f:\n",
        "    for linha in rdd_dados:\n",
        "        f.write(str(linha) + '\\n')\n"
      ],
      "metadata": {
        "id": "Ey0RL8v6Iuk7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "def build_tuple(s):\n",
        "  fields = s.split(\";\")\n",
        "  sg_uf = fields[4]\n",
        "  tp_dependendia = int(fields[15]) if fields[15] else 0\n",
        "  return ((sg_uf, tp_dependendia), 1)\n",
        "\n",
        "rdd_quantidade = rdd_filtered.map(build_tuple)\n",
        "rdd_quantidade_re = rdd_quantidade.reduceByKey(lambda x,y: x + y)\n",
        "rdd_ordenado = rdd_quantidade_re.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "\n",
        "rdd_ordenado.collect()\n",
        "rdd_dados = rdd_ordenado.collect()\n",
        "\n",
        "\n",
        "with open('Quantidade_Por_UF_Dependencia.txt', 'w') as f:\n",
        "    for linha in rdd_dados:\n",
        "        f.write(str(linha) + '\\n')"
      ],
      "metadata": {
        "id": "2PLgbxa-IwNO"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}